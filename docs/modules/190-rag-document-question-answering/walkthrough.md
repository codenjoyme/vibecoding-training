# RAG: Document Question Answering - Hands-on Walkthrough

In this walkthrough, you'll build a Retrieval-Augmented Generation (RAG) system - the foundation of modern AI applications that answer questions from your documents. You'll learn how AI "reads" documents by converting them to vectors, searches semantically (by meaning, not keywords), and generates accurate answers grounded in source material.

## Prerequisites

- Completed **Module 180: DIAL Integration with Python and Langchain**
- Completed **Module 185: Prompt Templates for Dynamic Queries**
- Python virtual environment set up in `work/180-task`
- DIAL API credentials with embedding model configured in `.env`
- Virtual environment activated (showing `.venv` prefix in terminal)

## Quick Start: Install FAISS and Copy Script

This module reuses the Python environment from Module 180. Only need to install FAISS (vector search library) and copy the RAG example:

**Windows:**
```powershell
cd work\180-task
.\.venv\Scripts\Activate.ps1
pip install faiss-cpu
Copy-Item ..\..\docs\modules\190-rag-document-question-answering\tools\rag.py .
```

**macOS/Linux:**
```bash
cd work/180-task
source .venv/bin/activate
pip install faiss-cpu
cp ../../docs/modules/190-rag-document-question-answering/tools/rag.py .
```

Verify your `.env` file includes embedding model configuration:
```env
AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT=text-embedding-ada-002
```

Now skip to **Part 4: Running the RAG Example** to see it in action, or continue with Part 1-2 to understand RAG architecture first.

---

## Part 1: Understanding RAG Architecture

Before coding, let's understand what RAG does and why it matters.

**The Problem: AI Hallucination**
```python
# Without RAG - AI might make things up
query = "What is our company's vacation policy?"
response = llm.invoke(query)
# Response: AI invents a policy it doesn't know
```

**The Solution: RAG**
1. **Retrieval:** Search your documents for relevant information
2. **Augmented:** Add that context to the prompt
3. **Generation:** AI generates answer based on provided context

**RAG Flow:**
```
User Question â†’ Vector Search â†’ Find Relevant Docs â†’ Add to Prompt â†’ AI Answer
```

**Why RAG Works:**
- AI only answers from documents you provide (no hallucinations)
- Searches by **meaning** (semantic), not just keywords
- Scales to millions of documents
- Updates knowledge without retraining AI models
- Can cite sources for answers

**Real-World Example:**
- Question: "What do cats like?"
- System searches documents for semantic matches
- Finds: "A cat is a domestic animal... Cats love to sleep and play."
- AI generates answer using that context
- Result: Factual answer grounded in your documents

## Part 2: Understanding Vector Embeddings

RAG uses embeddings to understand document meaning.

**What are Embeddings?**
- Documents converted to numbers (vectors) representing meaning
- Similar meanings = similar vectors (mathematically close)
- Example: 
  - "cat" and "kitten" â†’ similar vectors (related concepts)
  - "cat" and "Python programming" â†’ different vectors (unrelated)

**How Semantic Search Works:**
```
Document: "Cats love to sleep" â†’ Embedding: [0.2, 0.8, 0.3, ...]
Query: "What do felines like?" â†’ Embedding: [0.25, 0.75, 0.35, ...]
Similarity Score: 0.92 (very similar) â†’ Document is relevant!
```

**Key Insight:** The AI model (embedding model) learned that "cats" and "felines" mean similar things, even though the words are different. Traditional keyword search would miss this!

## Part 3: Understanding the RAG Code
   
   **What is FAISS?**
   - Facebook AI Similarity Search
   - High-performance vector database
   - Finds similar documents extremely fast
   - Works locally (no cloud database needed)

4. Verify your `.env` has embedding model:
   ```env
   AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT=text-embedding-ada-002
   ```
   
   If missing, add this line to your `.env` file.

**What's already here from Module 180:**
- âœ… Python virtual environment
- âœ… Langchain and dependencies
- âœ… `.env` file with DIAL credentials
- âœ… `color.py` utility

**What we just added:**
- âœ… FAISS vector database
- âœ… `rag.py` - The RAG example script

## Part 3: Understanding the RAG Code

Open `rag.py` and examine the structure:

**Step 1: Setup Models**
```python
# LLM for generating answers
llm = AzureChatOpenAI(...)

# Embedding model for converting text to vectors
embeddings = AzureOpenAIEmbeddings(
    model=os.getenv("AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT"),
    ...
)
```
- `llm`: Generates natural language answers
- `embeddings`: Converts documents and queries to vectors

**Step 2: Prepare Documents**
```python
docs = [
    "A cat is a domestic animal... Cats love to sleep and play.",
    "A dog is a human's best friend... Dogs guard the house...",
    "Python is a programming language..."
]

documents = [Document(page_content=doc) for doc in docs]
```
- Start with simple text strings
- Convert to Langchain `Document` objects
- In real apps, load from PDFs, databases, websites

**Step 3: Create Vector Store**
```python
vector_store = FAISS.from_documents(documents, embeddings)
```
- `FAISS.from_documents()`: Converts all documents to embeddings and indexes them
- Happens once when building knowledge base
- Creates searchable vector database

**Step 4: Search for Relevant Documents**
```python
results_with_scores = vector_store.similarity_search_with_score(question, k=3)
```
- Takes question, converts to embedding
- Finds `k=3` most similar documents
- Returns documents with similarity scores

**Step 5: Filter by Relevance**
```python
relevance_threshold = 0.5
relevant_docs = []

for doc, score in results_with_scores:
    if score < relevance_threshold:  # Lower score = more similar
        relevant_docs.append(doc)
```
- Similarity scores: 0.0 (perfect match) to 1.0+ (unrelated)
- Filter out irrelevant documents
- Prevents adding noise to AI context

**Step 6: Generate Answer with Context**
```python
context = "\n\n".join([doc.page_content for doc in relevant_docs])

prompt = f"""Use the following context to answer the question.

Context: 
{context}

Question: {question}

Answer:"""

response = llm.invoke(prompt)
```
- Combine relevant documents into context string
- Build prompt with context + question
- AI generates answer using provided information

## Part 4: Running Your RAG System

1. Run the RAG example:
   ```powershell
   python rag.py
   ```

2. Observe the output:
   ```
   ======================================
   Question
   ======================================
   What do you know about animals?
   
   ======================================
   Searching for relevant documents
   ======================================
   ðŸ“„ Document found (similarity: 0.75): A cat is a domestic animal...
   ðŸ“„ Document found (similarity: 0.72): A dog is a human's best friend...
   âŒ Document filtered out (similarity: 0.35): Python is a programming language...
   
   ======================================
   Found context
   ======================================
   A cat is a domestic animal of the feline family. Cats love to sleep and play.
   
   A dog is a human's best friend. Dogs guard the house and love to walk.
   
   ======================================
   Response
   ======================================
   Based on the context, I can tell you about two domestic animals: cats and dogs. 
   Cats are feline animals that love sleeping and playing. Dogs are considered 
   human's best friend, they guard houses and enjoy walking.
   ```

**What Happened:**
- Query converted to embedding vector
- System found 2 relevant documents about animals
- Filtered out Python document (irrelevant)
- AI generated answer using only relevant context
- Response is factual, grounded in documents

## Part 6: Experimenting with Different Questions

Let's test how the system handles various queries.

1. Open `rag.py` and find the test question at the bottom:
   ```python
   ask_question("What do you know about animals?")
   ```

2. Try different questions:
   ```python
   ask_question("Tell me about pets")
   # Should find cat and dog documents
   
   ask_question("What is Python?")
   # Should find programming document
   
   ask_question("How do I bake a cake?")
   # Should find NO relevant documents - AI says "I don't have information"
   ```

3. Run after each change:
   ```powershell
   python rag.py
   ```

**Key Observations:**
- "pets" finds animal docs (semantic similarity, not exact keyword)
- Questions outside document scope â†’ AI says it doesn't know
- This prevents hallucinations!

## Part 7: Adding Your Own Documents

Now let's build a RAG system with your own knowledge base.

1. Create `custom_rag.py`:
   ```python
   from color import header
   header("Custom RAG System", "cyan")
   
   import os
   from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
   from langchain_community.vectorstores import FAISS
   from langchain.schema import Document
   from dotenv import load_dotenv
   
   load_dotenv()
   
   llm = AzureChatOpenAI(
       azure_deployment = os.getenv("AZURE_OPENAI_API_DEPLOYMENT"),
       api_version      = os.getenv("AZURE_OPENAI_API_VERSION"),
       api_key          = os.getenv("AZURE_OPENAI_API_KEY"),
       azure_endpoint   = os.getenv("AZURE_OPENAI_ENDPOINT"),
       max_tokens       = 1000,
       temperature      = 0
   )
   
   embeddings = AzureOpenAIEmbeddings(
       model=os.getenv("AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT"),
       deployment=os.getenv("AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT"),
       api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
       api_key=os.getenv("AZURE_OPENAI_API_KEY"),
       azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
   )
   
   # YOUR CUSTOM DOCUMENTS - Replace with your content!
   my_docs = [
       "Our company vacation policy: Employees receive 20 days paid vacation per year. Vacation must be approved by manager 2 weeks in advance.",
       "Remote work policy: Employees can work remotely up to 3 days per week. Full remote work requires VP approval.",
       "Expense reimbursement: Submit receipts within 30 days. Meals capped at $50 per day. Travel expenses require pre-approval.",
       "Meeting policy: Meetings should have agenda sent 24 hours before. Keep meetings under 1 hour when possible."
   ]
   
   documents = [Document(page_content=doc) for doc in my_docs]
   vector_store = FAISS.from_documents(documents, embeddings)
   
   def ask_company_policy(question):
       header("Question")
       print(question)
       
       results = vector_store.similarity_search_with_score(question, k=2)
       
       relevant_docs = [doc for doc, score in results if score < 0.5]
       
       if not relevant_docs:
           return "I don't have relevant policy information for that question."
       
       context = "\n\n".join([doc.page_content for doc in relevant_docs])
       
       prompt = f"""Answer the question based on company policy context below.
       
   Context:
   {context}
   
   Question: {question}
   
   Answer:"""
       
       response = llm.invoke(prompt)
       
       header("Answer")
       print(response.content)
       return response.content
   
   # Test queries
   ask_company_policy("How many vacation days do I get?")
   ask_company_policy("Can I work from home?")
   ask_company_policy("What's the meal expense limit?")
   ```

2. Run your custom RAG:
   ```powershell
   python custom_rag.py
   ```

3. Add your own documents:
   - Replace `my_docs` with your content
   - Could be: product documentation, meeting notes, code documentation
   - Try different question variations

## Part 8: Loading Documents from Files

Real applications load documents from files, not hardcoded strings.

1. Create a sample document file `knowledge.txt`:
   ```
   Product: SmartWatch Pro
   Features: Heart rate monitor, GPS tracking, 7-day battery, waterproof to 50m
   Price: $299
   Warranty: 2 years manufacturer warranty
   
   Product: FitBand Lite  
   Features: Step counter, sleep tracking, 14-day battery
   Price: $79
   Warranty: 1 year manufacturer warranty
   ```

2. Create `file_rag.py`:
   ```python
   from color import header
   header("File-based RAG", "green")
   
   import os
   from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
   from langchain_community.vectorstores import FAISS
   from langchain.schema import Document
   from dotenv import load_dotenv
   
   load_dotenv()
   
   llm = AzureChatOpenAI(
       azure_deployment = os.getenv("AZURE_OPENAI_API_DEPLOYMENT"),
       api_version      = os.getenv("AZURE_OPENAI_API_VERSION"),
       api_key          = os.getenv("AZURE_OPENAI_API_KEY"),
       azure_endpoint   = os.getenv("AZURE_OPENAI_ENDPOINT"),
       max_tokens       = 1000,
       temperature      = 0
   )
   
   embeddings = AzureOpenAIEmbeddings(
       model=os.getenv("AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT"),
       deployment=os.getenv("AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT"),
       api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
       api_key=os.getenv("AZURE_OPENAI_API_KEY"),
       azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
   )
   
   # Load documents from file
   with open("knowledge.txt", "r") as f:
       content = f.read()
   
   # Split by paragraphs (simple chunking)
   chunks = [chunk.strip() for chunk in content.split("\n\n") if chunk.strip()]
   
   documents = [Document(page_content=chunk) for chunk in chunks]
   
   header("Loaded Documents")
   print(f"Loaded {len(documents)} document chunks")
   
   vector_store = FAISS.from_documents(documents, embeddings)
   
   def ask_about_products(question):
       header("Question")
       print(question)
       
       results = vector_store.similarity_search_with_score(question, k=2)
       
       for doc, score in results:
           print(f"\nðŸ“„ Found (similarity: {1-score:.2f}):")
           print(doc.page_content[:100] + "...")
       
       relevant_docs = [doc for doc, score in results if score < 0.5]
       
       if not relevant_docs:
           return "No relevant product information found."
       
       context = "\n\n".join([doc.page_content for doc in relevant_docs])
       
       prompt = f"""Answer based on product information:
       
   {context}
   
   Question: {question}
   Answer:"""
       
       response = llm.invoke(prompt)
       
       header("Answer")
       print(response.content)
       return response.content
   
   ask_about_products("Which product has GPS?")
   ask_about_products("What's the cheapest option?")
   ask_about_products("How long is the warranty?")
   ```

3. Run the file-based RAG:
   ```powershell
   python file_rag.py
   ```

**Document Chunking:**
- Large documents split into smaller chunks
- Each chunk becomes separate vector
- Improves search relevance
- Balance: too small = loses context, too large = too much noise

## Part 9: Three Ways to Run Your RAG Scripts

RAG scripts require FAISS vector database. Here's how to run them in different environments.

### Method 1: Windows Native Execution

**Best for:** Daily development, debugging, quick iterations

**Prerequisites:**
- Completed Module 180 setup (venv exists in `work/180-task`)
- **FAISS installed** (see Part 3 - `pip install faiss-cpu`)
- `.env` file configured with embedding model

**Steps:**

1. Navigate to workspace:
   ```powershell
   cd work\180-task
   ```

2. Activate virtual environment:
   ```powershell
   .\.venv\Scripts\Activate.ps1
   ```

3. **Ensure FAISS is installed** (if not done in Part 3):
   ```powershell
   pip install faiss-cpu
   ```

4. Run RAG scripts:
   ```powershell
   python rag.py
   python custom_rag.py
   python file_rag.py
   ```

**Advantages:**
- âœ… Fastest execution (no overhead)
- âœ… Easy debugging with IDE
- âœ… Can inspect vector store in memory

**Important:** FAISS must be installed manually with `pip install faiss-cpu` for native execution!

### Method 2: Linux/macOS Native Execution

**Best for:** Development on Linux/macOS systems

**Prerequisites:**
- Linux/macOS operating system
- Completed Module 180 setup (venv exists with dependencies)
- **FAISS installed** (see Part 3)
- Scripts exist in `work/python-ai-workspace`

**Steps:**

1. Navigate to workspace:
   ```bash
   cd work/180-task
   ```

2. Activate virtual environment:
   ```bash
   source .venv/bin/activate
   ```

3. **Ensure FAISS is installed** (if not done in Part 3):
   ```bash
   pip install faiss-cpu
   ```

4. Run RAG scripts:
   ```bash
   python3 rag.py
   python3 custom_rag.py
   python3 file_rag.py
   ```

**Advantages:**
- âœ… Fastest execution on Linux/macOS
- âœ… Easy debugging with native tools
- âœ… Can inspect vector store in memory

**Important:** FAISS must be installed manually with `pip install faiss-cpu` for native execution!

**Note:** If environment not set up yet, run Module 180's installation script first:
```bash
cd docs/modules/180-dial-langchain-python-integration/tools
./install-python-linux.sh
```

### Method 3: Docker Deployment (Production-Ready with FAISS)

**Best for:** Reproducible RAG execution, automatic FAISS installation

**Prerequisites:**
- Docker Desktop installed and running
- Scripts exist in `work/180-task`

**Steps:**

1. Navigate to Module 180 tools directory (reuse Docker scripts):
   ```powershell
   cd docs\modules\180-dial-langchain-python-integration\tools
   ```

2. Run RAG scripts with **FAISS auto-installation**:
   
   **Windows:**
   ```powershell
   .\install-python-docker.ps1 -Script "rag.py" -ExtraPackages "faiss-cpu"
   .\install-python-docker.ps1 -Script "custom_rag.py" -ExtraPackages "faiss-cpu"
   .\install-python-docker.ps1 -Script "file_rag.py" -ExtraPackages "faiss-cpu"
   ```
   
   **Linux/macOS:**
   ```bash
   ./install-python-docker.sh rag.py faiss-cpu
   ./install-python-docker.sh custom_rag.py faiss-cpu
   ./install-python-docker.sh file_rag.py faiss-cpu
   ```

3. What happens:
   - **First build with FAISS**: ~130-150 seconds (base image + FAISS installation)
   - Base langchain environment cached (~120 seconds)
   - FAISS layer cached separately (~10-30 seconds)
   - **Subsequent runs with same packages**: ~1-3 seconds (full cache hit)
   - **Subsequent runs with different packages**: ~5-30 seconds (rebuild FAISS layer only)

**Docker Build Layers:**
```
Step 1-6: Base image + langchain         [CACHED: ~120s first time]
Step 7:   Extra packages (faiss-cpu)     [CACHED: ~10-30s first time]
Step 8:   Run script                      [EXECUTES: ~1-5s]
```

**Advantages:**
- âœ… **FAISS auto-installed** - no manual `pip install` needed!
- âœ… Perfect reproducibility
- âœ… Layer caching: FAISS cached separately from base environment
- âœ… Change script without rebuilding (~1-3 seconds)
- âœ… Ideal for CI/CD with FAISS dependency

**Important Note:** The `-ExtraPackages` parameter is **required** for RAG scripts in Docker because FAISS is not included in the base image. This parameterization allows the base image to remain generic and reusable across all modules!

### Method Comparison for RAG

| Scenario | Recommended Method | Notes |
|----------|-------------------|-------|
| Developing RAG scripts | Method 1: Windows Native | Fastest, requires manual `pip install faiss-cpu` |
| Testing RAG in Linux | Method 3: Docker with FAISS | Auto-installs FAISS via `-ExtraPackages` |
| Sharing RAG with team | Method 3: Docker with FAISS | Team doesn't need to install FAISS manually |
| CI/CD for RAG | Method 3: Docker with FAISS | Reproducible, automated FAISS installation |
| Production RAG deployment | Method 3: Docker with FAISS | Isolated, reproducible environment |

### Understanding Extra Packages Parameter

**Why `-ExtraPackages`?**
- Base Docker image (Module 180) contains only: langchain, langchain-openai, python-dotenv
- Each module might need additional packages (RAG needs FAISS, future modules might need pandas, numpy, etc.)
- Instead of creating separate Docker images per module, we parameterize the build

**How it works:**
```dockerfile
# Dockerfile excerpt
ARG EXTRA_PACKAGES=""
RUN if [ -n "$EXTRA_PACKAGES" ]; then \
        pip3 install $EXTRA_PACKAGES; \
    fi
```

**Invoking from PowerShell:**
```powershell
.\install-python-docker.ps1 -Script "rag.py" -ExtraPackages "faiss-cpu pandas numpy"
```

**Result:**
- Base langchain environment: cached (no rebuild)
- FAISS + pandas + numpy: installed in separate layer (cached after first run)
- Your script: runs with all dependencies available

**Pro Tip:** Once you run Docker with `-ExtraPackages "faiss-cpu"`, the FAISS layer is cached. Subsequent RAG script runs with the same packages take only ~1-3 seconds!

## Part 10: Adjusting Relevance Threshold

Fine-tune how strict your RAG system is about relevance.

1. In your RAG scripts, find:
   ```python
   relevance_threshold = 0.5
   ```

2. Experiment with different values:
   - `0.3`: Very strict - only highly relevant docs
   - `0.5`: Balanced - good default
   - `0.7`: Permissive - includes loosely related docs
   - `1.0`: Very permissive - almost everything

3. Test with edge cases:
   ```python
   # Question somewhat related to documents
   ask_question("What animals make good companions?")
   # Try thresholds: 0.3 vs 0.7 and see difference
   ```

**Threshold Tuning Tips:**
- Start with 0.5
- If answers say "I don't know" too often â†’ increase threshold
- If answers include irrelevant info â†’ decrease threshold
- Monitor score distributions in your logs

## Success Criteria

âœ… Installed FAISS vector database successfully  
âœ… Ran rag.py and understood RAG workflow  
âœ… Tested different questions and observed semantic search  
âœ… Created custom_rag.py with your own documents  
âœ… Built file_rag.py that loads from text files  
âœ… Understand embeddings convert text to searchable vectors  
âœ… Can filter results by relevance threshold  
âœ… Recognize when RAG finds vs doesn't find relevant context  
âœ… Know difference between RAG and simple AI queries  

## Troubleshooting

### Issue: "No module named 'faiss'"

**Solution:**
```powershell
# Ensure virtual environment is activated
pip install faiss-cpu

# If still fails, try:
pip install faiss-cpu --upgrade
```

### Issue: Embedding model authentication error

**Solution:**
- Check `.env` has `AZURE_OPENAI_API_EMBEDDING_DEPLOYMENT` set
- Verify embedding model name matches DIAL available models
- Try: `text-embedding-ada-002` or `text-embedding-3-small`
- Ensure same API key works for both LLM and embeddings

### Issue: All documents filtered out (no relevant docs found)

**Solution:**
- Increase relevance threshold: `relevance_threshold = 0.7`
- Check documents actually contain information related to query
- Verify embeddings model is loading correctly
- Print similarity scores to see actual values:
  ```python
  for doc, score in results_with_scores:
      print(f"Score: {score}, Doc: {doc.page_content[:50]}")
  ```

### Issue: RAG answers include information not in documents

**Solution:**
- AI might still generate beyond context if prompt unclear
- Strengthen prompt instructions:
  ```python
  prompt = """ONLY use the context below. If the answer isn't in the context, 
  say "I don't have that information."
  
  Context: {context}
  Question: {question}"""
  ```
- Set temperature to 0 for more deterministic responses

### Issue: Slow embedding generation for large document sets

**Solution:**
- Create embeddings once, save vector store:
  ```python
  vector_store.save_local("my_vector_store")
  # Later, load instead of recreating:
  vector_store = FAISS.load_local("my_vector_store", embeddings)
  ```
- Chunk large documents before embedding
- Consider batch processing for thousands of documents

### Issue: Semantic search finds wrong documents

**Solution:**
- Document chunks might be too large (losing specificity)
- Try smaller chunks: split by sentences or paragraphs
- Increase `k` parameter to search more documents
- Check if documents actually contain relevant info
- Consider adding metadata filters for multi-topic knowledge bases

## When to Use

**Use RAG:**
- Q&A systems over company documentation
- Chatbots that cite specific sources
- Document search with AI summarization
- Support systems requiring factual accuracy
- Any application where you need provenance (source tracking)
- Reducing hallucinations by grounding in documents

**Don't Use RAG:**
- Creative writing tasks (want imagination, not facts)
- General knowledge questions (AI training data sufficient)
- Simple keyword search (traditional search faster)
- When all context fits in prompt (no need for retrieval)

## Next Steps

Outstanding! You've built a working RAG system - the architecture powering most modern AI applications like chatbots, document assistants, and search systems.

**What you've learned:**
- RAG combines retrieval + generation for grounded answers
- Embeddings enable semantic (meaning-based) search
- Vector databases make similarity search efficient
- Relevance filtering prevents irrelevant context
- RAG prevents hallucinations by constraining to documents

**Advanced RAG topics to explore:**
- Multi-document RAG with metadata filtering
- Hybrid search (combining keyword + semantic)
- Re-ranking retrieved results for better relevance
- Streaming responses for better UX
- Production vector databases (Pinecone, Weaviate, Chroma)

**Practical projects to build:**
- Company policy Q&A bot
- Code documentation assistant  
- Meeting notes search with summarization
- Research paper analyzer
- Customer support knowledge base

---

**Key Takeaway:** RAG is the bridge between AI's language understanding and your specific knowledge. By retrieving relevant context and augmenting prompts, you build AI applications that answer accurately from your documents without hallucination - the foundation of trustworthy AI systems.
